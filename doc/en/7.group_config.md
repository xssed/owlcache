[<p align="right">->Back to directory</p>](0.directory.md)

# Cluster Configuration
* HTTP
Note: All cluster configuration related operations require authentication.

## Get all the nodes in the cluster

<table>
    <tr>
        <td>parameter name</td>
        <td>parameter value</td>
    </tr>
    <tr>
        <td>cmd</td>
        <td>getall</td>
    </tr>
    <tr>
        <td>token</td>
        <td>token value</td>
    </tr> 
</table> 
 
~~~shell
http://127.0.0.1:7721/server/?cmd=getall&token=451ec65b1dcafe54459eda5db3caca7d
~~~
<br>

Response result example:
~~~shell
{
    "Cmd": "getall",
    "Status": 200,
    "Results": "SUCCESS",
    "Address": "",
    "Data": [
        {
            "Cmd": "",
            "Address": "http://127.0.0.1:7721",
            "Pass": "shi!jie9he?ping6",
            "Token": "cd2a7809d92157f2330669a0bf079799"
        },
        {
            "Cmd": "",
            "Address": "http://127.0.0.1:7723",
            "Pass": "shi!jie9he?ping6",
            "Token": "cd2a7809d92157f2330669a0bf079799"
        },
        {
            "Cmd": "",
            "Address": "http://127.0.0.1:7725",
            "Pass": "shi!jie9he?ping6",
            "Token": "cd2a7809d92157f2330669a0bf079799"
        }
    ]
}
~~~


## Get the configuration information of a node in the cluster

<table>
    <tr>
        <td>parameter name</td>
        <td>parameter value</td>
    </tr>
    <tr>
        <td>cmd</td>
        <td>get</td>
    </tr>
    <tr>
        <td>address</td>
        <td>Node address(full)</td>
    </tr> 
    <tr>
        <td>token</td>
        <td>token value</td>
    </tr> 
</table> 
 
~~~shell
http://127.0.0.1:7721/server/?cmd=get&address=http://127.0.0.1:7723&token=451ec65b1dcafe54459eda5db3caca7d
~~~
<br>

Response result example:
~~~shell
{
    "Cmd": "get",
    "Status": 200,
    "Results": "SUCCESS",
    "Address": "http://127.0.0.1:7723",
    "Data": {
        "Cmd": "",
        "Address": "http://127.0.0.1:7723",
        "Pass": "shi!jie9he?ping6",
        "Token": "cd2a7809d92157f2330669a0bf079799"
    }
}
~~~  

## Add a node's configuration information

<table>
    <tr>
        <td>parameter name</td>
        <td>parameter value</td>
    </tr>
    <tr>
        <td>cmd</td>
        <td>add</td>
    </tr>
    <tr>
        <td>address</td>
        <td>Node address(full)</td>
    </tr> 
    <tr>
        <td>pass</td>
        <td>Node connection password</td>
    </tr> 
    <tr>
        <td>token</td>
        <td>token value</td>
    </tr> 
</table> 
 
~~~shell
http://127.0.0.1:7721/server/?cmd=add&address=http://127.0.0.1&pass=shi!jie9he?ping6&token=b755c07d4d59a3da38795bccebaaeefd
~~~
<br>

Response result example:
~~~shell
{
    "Cmd": "",
    "Status": 200,
    "Results": "SUCCESS",
    "Address": "http://127.0.0.1",
    "Data": null
}
~~~

## Delete the configuration information of a node  

<table>
    <tr>
        <td>parameter name</td>
        <td>parameter value</td>
    </tr>
    <tr>
        <td>cmd</td>
        <td>delete</td>
    </tr>
    <tr>
        <td>address</td>
        <td>Node address(full)</td>
    </tr> 
    <tr>
        <td>token</td>
        <td>token value</td>
    </tr> 
</table> 
 
~~~shell
http://127.0.0.1:7721/server/?cmd=delete&address=http://127.0.0.1&token=451ec65b1dcafe54459eda5db3caca7d
~~~
<br>

Response result example:
~~~shell
{
    "Cmd": "",
    "Status": 200,
    "Results": "SUCCESS",
    "Address": "http://127.0.0.1",
    "Data": null
}
~~~

# Cluster Configuration  
* The Gossip protocol is a solution for the ultimate consistency of data.
* When GroupWorkMode is set to gossip, the cluster information needs to be configured at boot time, but the configuration is easier and easier than owlcache mode.
* When GroupWorkMode is set to gossip, the data of each node is automatically updated to the latest, although there is a delay, but the time is very short.    

>Note: When the gossip cluster, the node exchanges data and needs to be authenticated. You need to set the value of the Pass option in the owlcache.conf file to a uniform password.
>Note: When a node is down or "brain split", other nodes in the cluster update the data. The node of the downtime reconnects into the cluster and the data is not updated (subsequent versions will solve this problem).  

>Suppose there are now three owlcache services: 127.0.0.1: 7721, 127.0.0.1:7723, 127.0.0.1:7725. Set the GroupWorkMode option in the owlcache.conf file to gossip.

We start the node `127.0.0.1:7721`, the output of the startup information is as follows (for example:)
~~~shell
......
2019/05/16 12:07:57 [DEBUG] memberlist: Using dynamic bind port 49980
Mark : local member 10.0.85.2:49980
~~~  
So we know the first node gossip service information `10.0.85.2:49980`ã€‚  

Now we write the file `server_group_config.json`
~~~shell
[
	{
		"Cmd": "",
		"Address": "10.0.85.2:49980",
		"Pass": "",
		"Token": ""
	}
]
~~~  
Then put it into the data file directory of node 2 `127.0.0.1:7723` (depending on the setting of the DBfile option in the configuration file), and then start the node.  
Also put this configuration file into the data file directory of node three `127.0.0.1:7725`, and then start the node.  
In this way, the cluster is configured. Node 2 and Node 3 will automatically contact each other. Assume that the node is down now, and node two and node three will still maintain communication and update data.   
